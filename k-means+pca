import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error
from plotnine import *
import numpy as np

from sklearn.model_selection import train_test_split, KFold, LeaveOneOut
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import make_column_transformer
from sklearn.mixture import GaussianMixture

%matplotlib inline

from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor # Decision Tree
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # random forest
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor # gradient boosting
from sklearn.linear_model import LinearRegression

# preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer

# model validation
from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv
from sklearn.model_selection import LeaveOneOut #LOO cv

# performance
from sklearn.metrics import accuracy_score, confusion_matrix,\
 f1_score, recall_score, precision_score, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score


# models
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor # Decision Tree
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # random forest
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor # gradient boosting
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression
from sklearn.calibration import calibration_curve
from sklearn.naive_bayes import GaussianNB, CategoricalNB


# preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer

# model validation
from sklearn.model_selection import train_test_split # simple TT split cv
from sklearn.model_selection import KFold # k-fold cv
from sklearn.model_selection import LeaveOneOut #LOO cv
from sklearn.model_selection import GridSearchCV

from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestNeighbors

# performance
import warnings
warnings.filterwarnings('ignore')
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer #Z-score variables, Polynomial


df = pd.read_csv("https://raw.githubusercontent.com/quintinaphan/mgsc310-spotify/refs/heads/main/Spotify%20Most%20Streamed%20Songs.csv")
df.head()

df.isnull().sum()

df.dropna(inplace = True)
df.reset_index(inplace = True)
df.isnull().sum()

df = df[df["track_name"] != "Love Grows (Where My Rosemary Goes)"]


#kmeans
predictors = ["energy_%", "danceability_%", "valence_%"]

X = df[predictors]
# notice that there is no y

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. Create empty model
km = KMeans(n_clusters = 2)

# 3. Fit model + predict
labels = km.fit_predict(X_scaled)

# 4. Assess model
silhouette_avg = silhouette_score(X_scaled, labels)
print(silhouette_avg)

# Add cluster labels to the original DataFrame
X["clusters"] = labels

(ggplot(X, aes(x="energy_%", y="danceability_%", color="factor(clusters)")) +
      geom_point() + theme_minimal() +
      labs(x="Energy", y="Danceability", title="Energy vs Danceability KMeans K = 2",
           color="Clusters"))

table1 = X.groupby('clusters').mean()
print(table1)

(ggplot(X, aes(x="energy_%", y="valence_%", color="factor(clusters)")) +
      geom_point() + theme_minimal() +
      labs(x="Energy", y="Valence", title="Energy vs Valence KMeans K=2",
           color="Clusters"))

(ggplot(X, aes(x="valence_%", y="danceability_%", color="factor(clusters)")) +
      geom_point() + theme_minimal() +
      labs(x="Valence", y="Danceability", title="Valence vs Danceability KMeans K = 2",
           color="Clusters"))

#gmm
predictors = ["energy_%", "danceability_%", "valence_%"]

X = df[predictors]
# notice that there is no y

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

gmm = GaussianMixture(n_components = 2)

# 3. Fit model + predict
labels = gmm.fit_predict(X_scaled)

silhouette_avg = silhouette_score(X_scaled, labels)
print(silhouette_avg)

# Add cluster labels to the original DataFrame
X["clusters2"] = labels

(ggplot(X, aes(x="energy_%", y="danceability_%", color="factor(clusters2)")) +
      geom_point() + theme_minimal() +
      labs(x="Energy", y="Danceability", title="GMM Clustering Results for K = 2",
           color="Clusters"))

# define features
feats = ["danceability_%", "valence_%", "energy_%", "acousticness_%", "instrumentalness_%", "liveness_%", "speechiness_%"]
spotify_feats = df[feats]

spotify_feats.dropna(inplace = True)
spotify_feats.reset_index(inplace = True, drop = True)

# z-score
scaler = StandardScaler()
spotify_feats = scaler.fit_transform(spotify_feats)

pca = PCA()
pca.fit(spotify_feats)

# scree plot
pcaDF = pd.DataFrame({"expl_var" :
                      pca.explained_variance_ratio_,
                      "pc": range(1,8),
                      "cum_var":
                      pca.explained_variance_ratio_.cumsum()})

(ggplot(pcaDF, aes(x = "pc", y = "expl_var"))
  + geom_line()
  + geom_point()
  + labs(x = "PCs", y = "Proportion of Explained Variance", title = "Scree Plot")
  + theme_bw())

# cumvar plot
(ggplot(pcaDF, aes(x = "pc", y = "cum_var"))
  + geom_line()
  + geom_point()
  + geom_hline(yintercept = 0.95, color = "red", linetype = "dashed")
  + labs(x = "Number of PCs", y = "Cumulative Proportion of Explained Variance", title = "Cumulative Variance Plot")
  + theme_minimal())

# define popularity based on streams
df["streams"] = df["streams"].astype(int)
df["popularity"] = (df["streams"] > df["streams"].median()).astype(int)  # streams below and above the median as binary

feats = ["danceability_%", "valence_%", "energy_%", "acousticness_%", "instrumentalness_%", "liveness_%", "speechiness_%"]
X = df[feats]
y = df["popularity"]

# tts
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pca = PCA()

pca.fit(X_train)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pca = PCA(n_components = 7)
pca.fit(X_train)

# apply PCA transformation
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

lr = LogisticRegression()
lr.fit(X_train_pca,y_train)

y_pred_train = lr.predict(X_train_pca)
y_pred_test  = lr.predict(X_test_pca)

y_pred_train_prob = lr.predict_proba(X_train_pca)[:,1]
y_pred_test_prob  = lr.predict_proba(X_test_pca)[:,1]

# assess
print("Log Reg Using all PCs:")
#print("Train Acc       : ", accuracy_score(y_train, y_pred_train))
#print("Train Prescision: ", precision_score(y_train, y_pred_train))
#print("Train Recall    : ", recall_score(y_train, y_pred_train))
#print("Train F1        : ", f1_score(y_train, y_pred_train))
print("Train ROC AUC   : ", roc_auc_score(y_train, y_pred_train_prob))


#print("Test Acc        : ", accuracy_score(y_test, y_pred_test))
#print("Test Prescision : ", precision_score(y_test, y_pred_test))
#print("Test Recall     : ", recall_score(y_test, y_pred_test))
#print("Test F1         : ", f1_score(y_test, y_pred_test))
print("Test ROC AUC    : ", roc_auc_score(y_test, y_pr


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pca = PCA(n_components = 0.9)
pca.fit(X_train)

# apply PCA transformation
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

lr = LogisticRegression()
lr.fit(X_train_pca,y_train)

y_pred_train = lr.predict(X_train_pca)
y_pred_test  = lr.predict(X_test_pca)

y_pred_train_prob = lr.predict_proba(X_train_pca)[:,1]
y_pred_test_prob  = lr.predict_proba(X_test_pca)[:,1]

# assess
print("Log Reg Using 90% of PCs:")
#print("Train Acc       : ", accuracy_score(y_train, y_pred_train))
#print("Train Prescision: ", precision_score(y_train, y_pred_train))
#print("Train Recall    : ", recall_score(y_train, y_pred_train))
#print("Train F1        : ", f1_score(y_train, y_pred_train))
print("Train ROC AUC   : ", roc_auc_score(y_train, y_pred_train_prob))


#print("Test Acc        : ", accuracy_score(y_test, y_pred_test))
#print("Test Prescision : ", precision_score(y_test, y_pred_test))
#print("Test Recall     : ", recall_score(y_test, y_pred_test))
#print("Test F1         : ", f1_score(y_test, y_pred_test))
print("Test ROC AUC    : ", roc_auc_score(y_test, y_pre
